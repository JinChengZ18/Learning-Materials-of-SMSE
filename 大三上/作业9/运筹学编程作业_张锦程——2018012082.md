<center><font size='7'>运筹学编程作业<font/>



<center><font size='4'>张锦程&thinsp; 材84&thinsp; 2018012082<font/>



<center><font size='4'>编程语言：Python&emsp; version：3.7.7&emsp; date：2020/10/18&emsp;<font/>


​                     


---

##### 目录

[TOC]

##### 例题

考虑无约束优化问题 $min\ 𝑓(𝑥_1, 𝑥_2) = (1 − 𝑥_1)^2 + 2(𝑥_2 − 𝑥_1^2)^2$  。取初始点 $𝑥_0 = [0,0]^𝑇$，用 MATLAB 或者 python 编程实现以下 5 种下降算法求解（要求采用精确直线搜索），终止条件为 $‖∇𝑓(𝑥)‖_2 ≤ 10^{−10}$。给出每种算法的最优解和最优值，并画出采用不同算法时函数值随迭代次数增加的变化曲线。

要求：以文件压缩包方式提交该题，压缩包中应含有程序源代码、求解结果（包括最优解和函数变化曲线的原文件）以及说明文档（PDF），其中说明文档内容为简要说明每种算法的最优值、最优解以及函数值变化曲线。 

（1）$𝑙_1$, $𝑙_2$, $𝑙_∞$ 范数最速下降方法； 

（2）两种共轭梯度法（Fletcher-Reeves、Polak-Ribiere）

##### 



##### 源码部分：


```python
import matplotlib.tri as tri
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
import time
import copy
```

定义待优化问题

```python
def function_z(x, y): # 待优化的函数
    return (1-x)**2 + 2*(y-x**2)**2
def function_zsqrt(x, y):
    return ((1-x)**2 + 2*(y-x**2)**2)**0.25

def function_dz(x, y): # function_z 的导数矩阵, 也可由scipy求导
    return np.array([-2+2*x+8*x**3-8*x*y, 4*y-4*x**2])
def function_d2z(x, y): # function_z 的 Hessian 导数矩阵, 也可由scipy求二阶导
    return np.array([[2+24*x**2-8*y, -8*x], [-8*x, 4]])
```



###### 定义一维精确搜素方法

采用牛顿法


```python
def newton_optimization(point0,ε,D): # point0为初始点，ε为搜索精度, D为搜索方向
    point=np.array(point0); D=np.array(D); t=1000
    while (t>ε) or (t<-ε) :
        f  = function_z(point[0],point[1])
        df = np.dot(function_dz(point[0],point[1]),D)
        d2f= np.dot(D, np.dot(function_d2z(point[0],point[1]),D.T))
        if df != 0:
            t = -df/d2f
            point = point + t*D
        else:
            return point
    return point
```



###### 定义 l1 范数优化

 $l_1$ 范数 $||D||_1=\sum\limits^n_{i=1}|d_i|$ ：

$\hat{d_i}=\begin{cases}
sgn(-\frac{\part f(X)}{\part x_i})\ &if\ |\frac{\part f(X)}{\part x_i}|=||\nabla f(X)||_{\infty} \\
0 &if\ |\frac{\part f(X)}{\part x_i}|\neq||\nabla f(X)||_{\infty} \\
\end{cases}$


```python
def l1_optimization(point0): # point0 为待寻找优化方向的点
    # 维数
    dimension = len(point0)
    D = np.zeros(dimension)
    dz = function_dz(point0[0], point0[1])
    i  = np.argmax(abs(dz)) # 选定的优化坐标序号
    D[i]=-np.sign(dz[i])
    return D
```



###### 定义 l2 范数优化

 $l_2$ 范数 $||D||_1=\sqrt{\sum\limits^n_{i=1}|d_i|^2}$ ：

$\hat{d_i}=sgn(-\frac{\part f(X)}{\part x_i})|\frac{\part f(X)}{\part x_i}|/(||\nabla f(X)||_q)$


```python
def f_length(A): # 向量 A 的长度
    length2 = 0
    for i in range(len(A)):
        length2 += A[i]**2
    return length2**0.5

def l2_optimization(point0): # point0 为待寻找优化方向的点
    # 维数
    dimension = len(point0)
    D = np.zeros(dimension)
    dz= function_dz(point0[0], point0[1])
    D = -dz/f_length(dz)
    return D
```



######  定义 l∞ 范数优化

定义 $l_\infty$ 范数 $||D||_\infty=\max\limits_{1\leq i\leq n}|d_i|$ 优化：

$\hat{d_i}=sgn(-\frac{\part f(X)}{\part x_i}), \forall i$


```python
def linf_optimization(point0): # point0 为待寻找优化方向的点
    # 维数
    dimension = len(point0)
    D = np.zeros(dimension)
    dz= function_dz(point0[0], point0[1])
    for i in range(len(dz)):
        D[i] = -np.sign(dz[i])
    return D
```



###### Fletcher-Reeves & Polak-Ribiere 共轭梯度法

下面实现 Fletcher-Reeves 和 Polak-Ribiere 两种共轭梯度法：


```python
# Fletcher-Reeves共轭梯度法, f_nabla为先前一步的梯度绝对值,Di为前一步的优化方向
def FR_optimization(point0,f_nabla,Di):
    dz= function_dz(point0[0], point0[1])
    α = f_length(dz)**2/f_nabla**2 # f_nabla为∞时，α为0
    D = -dz + α*Di
    print(α)
    return D
```


```python
# Polak-Ribiere 共轭梯度法,vec_nabla为先前一步的梯度,Di为前一步的优化方向
def PR_optimization(point0,vec_nabla,Di):
    dz= function_dz(point0[0], point0[1])
    α = np.dot(dz,dz-vec_nabla)/f_length(vec_nabla)**2
    D = -dz + α*Di
    print(α)
    return D
```



###### 绘图函数

```python
def plot(name,f_values,f_nablas,f_points):
    min_d = -0.25; max_d = 1.25; n_d = 100

    x = np.linspace(min_d, max_d, n_d).flatten()
    y = np.linspace(min_d, max_d, n_d).flatten()
    X, Y = np.meshgrid(x, y)
    z = function_zsqrt(X, Y)

    plt.rcParams["figure.figsize"] = [22, 10]
    plt.rcParams["figure.dpi"] = 300
    plt.rcParams['font.size'] = '18' # 设置字体大小 
    plt.rcParams['lines.color'] = 'k'

    fig = plt.figure()
    grid = plt.GridSpec(8, 8, wspace=0.75, hspace=5)

    ax1=plt.subplot(grid[0:8,0:4]); ax2=plt.subplot(grid[0:4,4:8]); ax3=plt.subplot(grid[4:8,4:8])

    #画等高线图 ax1
    levels = np.arange(0., function_zsqrt(max_d+0.25, 0), 0.05)
    cmap = cm.get_cmap(name='coolwarm', lut=None)
    ax1.contourf(X,Y,z, levels=levels, cmap=cmap)
    ax1.contour(X,Y,z, levels=levels,
                  colors=['0.25', '0.75', '0.75', '0.75', '0.75'],
                  linewidths=[1.0, 0., 0., 0., 0.])
    ax1.set_xlabel(r'$x_1$'); ax1.set_ylabel(r'$x_2$')
    ax1.scatter([f_points[0][0]],[f_points[0][1]],color='',marker='o',edgecolors='r',s=100,label='Init Point')
    ax1.scatter([f_points[-1][0]],[f_points[-1][1]],color='',marker='o',edgecolors='b',s=100,label='End Point')
    
    ax1.text(min_d+0.05, min_d+0.1, r"$min\ f(𝑥_1,𝑥_2) = (1−𝑥_1)^2 + 2(𝑥_2−𝑥_1^2)^2$", fontsize=20)   
    legend1 = ax1.legend(fancybox=False,edgecolor='k',loc=2); frame1=legend1.get_frame(); frame1.set_alpha(1); frame1.set_facecolor('none')
    ax1.spines['top'].set_linewidth('2.0');ax1.spines['left'].set_linewidth('2.0')
    ax1.spines['bottom'].set_linewidth('2.0');ax1.spines['right'].set_linewidth('2.0')
    
    # 画箭头以表示优化过程
    for i in range(len(f_points)-1):
        ax1.arrow(f_points[i][0],f_points[i][1], f_points[i+1][0]-f_points[i][0], f_points[i+1][1]-f_points[i][1],
                    width=0.008/(i+1),length_includes_head=True,head_width=0.03/(i+1),head_length=0.08/(i+1),fc='r',ec='r')   
    
    # 画函数值随优化次数的变化情况
    ax2.plot(list(range(len(f_values))), f_values, 'r', label=name+" optimization")
    ax2.set_xlabel('Iterations Times'); ax2.set_ylabel('Function Values')
    ax2.legend(framealpha=0)
    ax2.spines['top'].set_linewidth('2.0');ax2.spines['left'].set_linewidth('2.0')
    ax2.spines['bottom'].set_linewidth('2.0');ax2.spines['right'].set_linewidth('2.0')
    
    # 画梯度绝对值随优化次数的变化情况
    ax3.plot(list(range(len(f_nablas))), f_nablas, 'r', label=name+" optimization")
    ax3.set_xlabel('Iterations Times'); ax3.set_ylabel('Gradient Absolute Values')
    ax3.legend(framealpha=0)
    ax3.spines['top'].set_linewidth('2.0');ax3.spines['left'].set_linewidth('2.0')
    ax3.spines['bottom'].set_linewidth('2.0');ax3.spines['right'].set_linewidth('2.0')
    
    plt.savefig(name+'迭代次数-函数值'+'.png')
    plt.show()
    
plot('示例',[1,0.7,0.2,0],[1,0.5,0.1,0],[[0,0],[0.4,0],[0.88,0.5],[1,1]])
```

![png](output_14_1.png)



###### 定义优化过程

下面开始定义优化过程，默认优化从 [0,0] 开始：


```python
init_point = [0,0]

def main(init_point,method_name,ε=10**(-10)):   # method_name为所使用的优化方法名称, init_point为优化初始点
    time_start=time.time() # 程序计时
    point = init_point
    f_values = [function_z(point[0],point[1])]  # 优化过程中所经历的函数值的集合
    f_nablas = [f_length(function_dz(point[0],point[1]))] # 优化过程中所经历的梯度绝对值的集合
    f_points = [point]  # 优化过程中所经历的点集
    t = float("inf"); f_nabla = float("inf"); Di = np.zeros(len(f_points)) # 步长和梯度和优化方向
    while f_nabla>ε:
        if method_name=='l1':
            D=l1_optimization(point0=point)
        if method_name=='l2':
            D=l2_optimization(point0=point)
        if method_name=='l∞':
            D=linf_optimization(point0=point)
        if method_name=='FR':
            D=FR_optimization(point0=point,f_nabla=f_nablas[-1],Di=Di)
        if method_name=='PR':
            D=PR_optimization(point0=point,vec_nabla=function_dz(f_points[-1][0],f_points[-1][1]),Di=Di)
        Di=copy.deepcopy(D)
        point = newton_optimization(point0=point,ε=ε,D=D)
        f_nabla = f_length(function_dz(point[0],point[1]))
        
        f_values.append(function_z(point[0],point[1]))
        f_nablas.append(f_nabla)
        f_points.append(point)
        t = f_length(f_points[-1]-f_points[-2])
        
    time_end=time.time()
    print('最优解: '+str(f_points[-1])); print('最优值: '+str(f_values[-1]))
    print('优化用时: '+str(time_end-time_start)); print("\n\n\n")
    plot(method_name,f_values,f_nablas,f_points)     
    return point
```


```python
point_l1 = main(init_point,'l1',ε=10**(-10))
point_l2 = main(init_point,'l2',ε=10**(-10))
point_linf=main(init_point,'l∞',ε=10**(-10))
point_FR = main(init_point,'FR',ε=10**(-10))
point_PR = main(init_point,'PR',ε=10**(-10))
```



###### 优化结果

$l_1$ 范数优化

```
最优解: [1. 1.]
最优值: 1.0892312857071719e-20
优化用时: 0.029986143112182617
```

![png](output_17_2.png)

$l_2$ 范数优化


    最优解: [1. 1.]
    最优值: 1.0892367478700591e-20
    优化用时: 0.03998136520385742


![png](output_17_4.png)

$l_2$ 优化的结果和 $l_1$ 在这里是完全一样的，这是由问题初始条件的特殊性所决定的，$l_2$ 每次的优化方向和上一次正交，$l_1$ 每次的优化方向只沿着坐标轴，而这里第一次的优化方向即 $[1,\ 0]$ 所以它们的寻优路线相同。



$l_\infty$ 范数优化

```
最优解: [1. 1.]
最优值: 9.46427809753135e-21
优化用时: 0.037994384765625
```


![png](output_17_6.png)

Fletcher-Reeves 共轭梯度法

```
最优解: [1. 1.]
最优值: 1.70317465220011e-21
优化用时: 0.7503437995910645
```

![FR迭代次数-函数值](运筹学编程作业_张锦程——2018012082.assets/FR迭代次数-函数值-1605945218727.png)

Polak-Ribiere 共轭梯度法

```
最优解: [1. 1.]
最优值: 4.965527361530408e-21
优化用时: 0.1954784393310547
```

![PR迭代次数-函数值](运筹学编程作业_张锦程——2018012082.assets/PR迭代次数-函数值.png)

从上图的结果可以看出，在优化的开始阶段，P-R 法和最速下降法的寻优路径相似，但是 P-R 法在接近极小值点的区域表现更好，只迭代了不到 100 次便收敛到了最优点，但是运算时间增大了 4 倍；F-R法的表现似乎不佳，在极值点附近收敛路径成为螺线，可能是函数中存在鞍点所致。







